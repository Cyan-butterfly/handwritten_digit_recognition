# 神经网络调参日志

## 2025-01-03：修复网络无法学习的问题

### 问题描述
网络完全无法学习，准确率一直停留在约11%左右（接近随机猜测的10%），且预测结果对所有样本都给出几乎相同的概率分布。

### 问题分析过程

1. 第一次尝试：修改损失函数实现
   - 修改：调整了交叉熵损失函数的batch size处理
   - 结果：无效，准确率仍然在11%左右
   - 经验：在调试神经网络时，应该首先确保数据预处理正确

2. 第二次尝试：更换激活函数
   - 修改：从sigmoid改为ReLU
   - 结果：无效，准确率仍然没有提升
   - 经验：激活函数的选择确实重要，但不是导致网络完全无法学习的根本原因

3. 第三次尝试：调整网络初始化
   - 修改：使用He初始化替代Xavier初始化
   - 结果：无效，网络仍然无法学习
   - 经验：初始化方法的选择要配合激活函数，但同样不是根本问题

4. 最终发现：数据归一化错误
   - 问题：输入数据的最大值只有0.003921568859368563
   - 原因：数据在加载时就被过度缩小了
   - 解决：直接使用keras.datasets.mnist.load_data()并正确归一化（除以255）
   - 结果：网络开始正常学习，准确率迅速提升到93%以上

### 最终解决方案

1. 数据预处理：
   ```python
   # 正确的数据加载和归一化
   (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
   x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
   x_test = x_test.reshape(-1, 784).astype('float32') / 255.0
   ```

2. 网络架构：
   - 使用ReLU激活函数
   - 使用He初始化（* np.sqrt(2.0 / input_size)）
   - 添加L2正则化防止过拟合

3. 训练参数：
   - 隐藏层：256个神经元
   - 学习率：0.1
   - 批量大小：128
   - 训练轮数：5轮（3000次迭代）

### 最终效果
- 训练集准确率：93.15%
- 测试集准确率：93.40%
- 损失从5.0150下降到0.4832
- 预测结果显示明确的类别区分

### 经验总结

1. 调试神经网络的优先级：
   - 第一优先级：确保数据预处理正确（尤其是归一化）
   - 第二优先级：检查网络架构和初始化
   - 第三优先级：调整超参数

2. 重要发现：
   - 数据的尺度对神经网络训练至关重要
   - 当网络完全无法学习时，很可能是数据预处理出了问题
   - 观察原始数据的统计特征（如最大值、最小值）很重要

3. 调试技巧：
   - 打印网络在训练过程中的预测分布
   - 观察损失值的变化趋势
   - 比较训练集和测试集的准确率差异

4. 代码改进：
   - 添加更多的日志输出
   - 保存中间状态用于调试
   - 使用更可靠的数据加载方法

### 后续优化方向
1. 增加训练轮数
2. 使用更大的隐藏层
3. 添加Dropout层防止过拟合
4. 实现动态学习率调整
5. 添加批归一化层
