# 项目记录

2025.1.1
在尝试理解深度学习入门：基于python的实现这本书的代码时，发现钻到细节里了，于是决定从顶层开始理解，买了淘宝的windsurf会员，开始了本项目的构建
1.用windsurf完成handwritten数字识别项目的关键代码构建，包括network等五个文件
2.跑起来代码，并利用AI做了训练日志的自动记录，免去了频繁的复制粘贴
3.理解代码结构和原理
1）首先尝试理解现有代码的结构
从核心组件开始：
网络结构（network.py）包括predict等函数
损失函数（loss.py）包括交叉熵函数与其导数
激活函数（activation.py）包括sigmoid和它的导数，和softmax
详细情况如下：
网络结构 (network.py):
init: 初始化网络参数（权重W1, W2和偏置b1, b2）
predict: 前向传播，计算预测结果
loss: 计算损失值
accuracy: 计算准确率
gradient: 反向传播，计算梯度
train: 训练函数，包含整个训练循环
激活函数 (activation.py):
sigmoid: 用于隐藏层的激活函数
sigmoid_grad: sigmoid的导数，用于反向传播
softmax: 用于输出层的激活函数
数据流程:
输入层 (784个神经元，对应28x28像素)
隐藏层 (50个神经元，使用sigmoid激活)
输出层 (10个神经元，使用softmax激活)

2）发现只看结构比较抽象，然后尝试理解network等文件的作用和关系
3）然后尝试理解重要方法，看到有不懂的地方，比如说交叉熵函数，找了一个youtube视频理解，
4）理解各个组件的作用和关系（**现在进度**）



此外
**学习到了一种项目探索方式**：
首先理解代码结构，这样可以建立整体认知
了解各个组件的作用和关系，为后续实验打下基础
然后实验和观察
有了基础理解后，实验会更有针对性，知道修改参数会影响哪些部分，能更好地解释实验结果
最后深入核心概念
有了实践经验，理论学习会更容易理解，可以将概念与实际效果对应，加深对原理的理解

明天计划：
1. 深入理解反向传播（1小时）

详细学习 gradient 方法的实现
理解链式法则在神经网络中的应用
手动计算一个简单例子的梯度传播过程
2. 实验和可视化（1小时）

修改网络参数进行对比实验：
尝试不同的隐藏层大小（25, 50, 100）
尝试不同的学习率（0.01, 0.1, 0.5）
添加训练过程可视化：
绘制损失函数随时间变化曲线
绘制准确率随时间变化曲线
3. 实际应用（1小时）

编写代码处理单个手写数字图片
可视化隐藏层学到的特征
分析模型对不同数字的识别能力
建议学习顺序
先从反向传播开始，因为这是最核心的概念
然后做实验，加深理解
最后是实际应用，巩固所学知识
学习目标
理解反向传播的原理和实现
掌握参数调整对模型的影响
能够实际应用模型进行预测